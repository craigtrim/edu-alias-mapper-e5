# ğŸ§­ edu-alias-mapper-e5
**Semantic Mapping of Institution Aliases â†’ Canonical Names Using Sentence Transformers + FAISS**

![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![CUDA](https://img.shields.io/badge/cuda-Enabled-brightgreen.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Framework](https://img.shields.io/badge/framework-PyTorch-red.svg)

---

## ğŸš€ Overview
**Aliasâ€“Label Retriever** is a GPU-accelerated pipeline that learns semantic relationships between institution aliases and canonical names. It fine-tunes a **SentenceTransformer** model on aliasâ†”label pairs (e.g., *"UdeG" â†’ "University of Guadalajara"*) and builds **FAISS** indexes for instant bi-directional retrieval.  
Built and trained on a **DGX (Sparx)** node, the system delivers lightning-fast semantic lookup across tens of thousands of entries.

---

## ğŸ§© Architecture
```
/alias-label-retriever
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Source datasets (.parquet, .csv)
â”‚ â”œâ”€â”€ processed/ # Cleaned, merged, or chunked data
â”‚ â””â”€â”€ cache/ # Temporary embeddings, staging data
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ base/ # Pretrained weights (e.g., intfloat/e5-large-v2)
â”‚ â””â”€â”€ trained/ # Fine-tuned SentenceTransformer checkpoints
â”‚
â”œâ”€â”€ faiss/ # Vector indexes for alias/label retrieval
â”‚ â”œâ”€â”€ labels.index
â”‚ â””â”€â”€ aliases.index
â”‚
â”œâ”€â”€ logs/ # Training and lookup logs
â”‚ â””â”€â”€ metrics/ # Epoch-level performance data
â”‚
â”œâ”€â”€ scripts/ # Executable Python modules
â”‚ â”œâ”€â”€ gpu_check.py
â”‚ â”œâ”€â”€ train_alias_label.py
â”‚ â””â”€â”€ test_lookup.py
â”‚
â””â”€â”€ notebooks/ # Optional Jupyter exploration
```

---

## âš™ï¸ Setup
### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/<your-username>/alias-label-retriever.git
cd alias-label-retriever
```

### 2ï¸âƒ£ Create and activate environment
```bash
conda create -n sparx python=3.11 -y
conda activate sparx
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121
pip install sentence-transformers faiss-cpu datasets accelerate pandas pyarrow
```
_(If your GPU supports CUDA, replace faiss-cpu with faiss-gpu)_

### ğŸ§  Training

Train the semantic model on your dataset:
```python
python scripts/train_alias_label.py
```

Outputs:
- Fine-tuned model â†’ models/trained/alias_label_e5/
- FAISS indexes â†’ faiss/aliases.index and faiss/labels.index
- Logs â†’ logs/training.log

### ğŸ” Inference / Lookup
Alias â†’ Canonical Label
```bash
python scripts/test_lookup.py --query "UdeG" --direction alias-to-label
```

Canonical Label â†’ Likely Aliases
```bash
python scripts/test_lookup.py --query "University of Guadalajara" --direction label-to-alias
```

Example output:
```
ğŸ” Query: UdeG
ğŸ“ˆ Top matches:
  1. University of Guadalajara                         (0.9761)
  2. Universidad de Guadalajara                        (0.9518)
  3. UdG                                              (0.9423)
```


### ğŸ§® GPU Verification
Before training, confirm CUDA readiness:
```bash
python scripts/gpu_check.py
```

Example log:
```bash
ğŸ§® Detected 1 CUDA device(s)
âœ… GPU[0] NVIDIA GB10 â€” 79.1 GB free / 119.7 GB total
ğŸ¯ GPU environment verified successfully
```

## ğŸ§± Core Stack
| Component | Purpose |
|------------|----------|
| **PyTorch** | Model training and GPU compute |
| **SentenceTransformers** | Fine-tuning & embedding generation |
| **FAISS** | Fast vector similarity search |
| **Hugging Face Datasets** | Efficient data streaming |
| **Pandas + Parquet** | Data I/O and preprocessing |
| **Accelerate** | GPU orchestration backend |

---

## ğŸ§¾ Example Dataset Schema
| Column | Type | Description |
|---------|------|-------------|
| `qid` | str | Wikidata or DBpedia ID |
| `label` | str | Canonical institution name |
| `alias` | str | Known alternative name or abbreviation |
| `description` | str | Optional metadata |
| `website` | str | Official site |

---

## ğŸ“Š Performance Snapshot
| Metric | Value |
|--------|--------|
| Epochs | 3 |
| Runtime | ~6.8 min |
| Throughput | 160 samples/sec |
| Final Loss | **0.388** |
| GPU | NVIDIA GB10 (119 GB) |

---

## ğŸ§  Future Extensions
- âš¡ Batch embedding + streaming loader for 10M+ entries  
- ğŸ§© ONNX export for low-latency inference  
- ğŸŒ Multilingual alias handling (e.g., English â†” Spanish)  
- ğŸ” Continuous fine-tuning from new institutional data  

---

## ğŸ‘¨â€ğŸ’» Author
**Craig Trim**  
âš™ï¸ *AI / Data Engineering â€“ Maryville University*  
ğŸ“ Built and trained on local DGX (**Sparx**)  

---

## ğŸªª License
MIT License â€” feel free to fork, modify, and extend.

---

## ğŸ§­ Quick Start Summary
| Step | Command |
|------|----------|
| ğŸ§© Verify GPU | `python scripts/gpu_check.py` |
| ğŸ§  Train Model | `python scripts/train_alias_label.py` |
| ğŸ” Query Index | `python scripts/test_lookup.py --query "UdeG"` |
| ğŸ§® Monitor GPU | `watch -n 1 nvidia-smi` |

## âš™ï¸ Technical Deep Dive

### ğŸ§  Model and Embedding Configuration
- **Base Model:** `intfloat/e5-large-v2` (Sentence Transformers)
- **Embedding Dimension:** 1024  
- **Fine-tuning Objective:** `MultipleNegativesRankingLoss`
- **Training Duration:** ~6.8 minutes (3 epochs)
- **Final Loss:** 0.3884  
- **Batch Size:** 64  
- **Framework:** PyTorch 2.3.0 + CUDA  
- **Precision:** Mixed-precision (AMP enabled)  
- **Device:** NVIDIA GB10 (119 GB VRAM)

This setup uses bi-encoder embeddingsâ€”alias and label strings are independently encoded into the same vector space, allowing cosine similarity to rank candidate matches. The `MultipleNegativesRankingLoss` encourages the model to bring correct aliasâ€“label pairs closer together and push unrelated pairs apart.

---

### âš™ï¸ FAISS Indexing
Two independent FAISS indexes are generated post-training:

| Index | Purpose | File Path |
|--------|----------|-----------|
| `labels.index` | Enables alias â†’ label lookup | `/faiss/labels.index` |
| `aliases.index` | Enables label â†’ alias lookup | `/faiss/aliases.index` |

Both use `IndexFlatIP` (inner-product similarity), which is equivalent to cosine similarity for normalized embeddings.  
Each index is fully memory-resident for sub-millisecond retrieval on GPU or CPU.

---

### ğŸ”© Training Flow Overview
1. **Load dataset** from Parquet â†’ Pandas (`data/raw/dbpedia_schools.parquet`)  
2. **Construct aliasâ†”label pairs**  
3. **Initialize** pretrained E5 model  
4. **Fine-tune** for 3 epochs using `MultipleNegativesRankingLoss`  
5. **Persist** fine-tuned model â†’ `models/trained/alias_label_e5/`  
6. **Encode** all aliases + labels â†’ dense embeddings  
7. **Write** FAISS indexes for instant semantic search  

---

### ğŸ§® Query Flow
1. User enters query (alias or label).  
2. Model encodes query â†’ 1024-D embedding.  
3. FAISS performs nearest-neighbor search in the relevant index.  
4. Returns ranked candidates with cosine similarity scores.  

Example:
```
Query: "UdeG"
Top-1 Match: "University of Guadalajara" (0.9761)
```


---

### ğŸ§° Libraries and Versions
| Library | Version | Role |
|----------|----------|------|
| `torch` | â‰¥ 2.3.0 | GPU compute |
| `sentence-transformers` | â‰¥ 3.0 | Model fine-tuning |
| `faiss-gpu` | â‰¥ 1.8.0 | Vector similarity search |
| `datasets` | â‰¥ 4.2.0 | Data streaming backend |
| `accelerate` | â‰¥ 0.26.0 | Trainer orchestration |
| `pandas` / `pyarrow` | latest | Data I/O |

---

### ğŸ§© Performance Considerations
- âš¡ **Encoding Speed:** ~30 k sentences/minute on GB10 GPU  
- ğŸ’¾ **Index Size:** ~400 MB per 50 k unique entries (float32)  
- ğŸ§  **Scalability:** Extendable to 10 M+ records via FAISS IVF or HNSW variants  
- ğŸ” **Retraining:** Fine-tuning can resume from any checkpoint in `/models/trained/`

---

### ğŸ›¡ï¸ Reproducibility
To reproduce identical results:
```bash
python scripts/gpu_check.py
python scripts/train_alias_label.py
python scripts/test_lookup.py --query "UdeG"
```
Model artifacts, logs, and indexes are version-controlled under their respective folders to ensure deterministic behavior across reruns.
